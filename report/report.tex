\documentclass[11pt, twocolumn, a4paper]{article}
\usepackage[a4paper, left = 2cm, right = 2cm, top = 2cm, bottom = 2cm]{geometry}
\usepackage[style = numeric, sorting = none, urldate = long]{biblatex}
\usepackage{listings}
\usepackage{subfig}
% \usepackage{amsmath}
% \usepackage{graphicx}


% \graphicspath{{./images/}}
\addbibresource{refs.bib}

\author{
    George Herbert\\
    \texttt{cj19328@bristol.ac.uk}
}

\title{Optimising and Parallelising d2q9-bgk.c}
\begin{document}

\maketitle

\begin{abstract}
    \texttt{d2q9-bgk.c} implements the Lattice Boltzmann methods (LBM) to simulate a fluid density on a lattice.
    This report outlines the techniques I utilised to optimise and parallelise \texttt{d2q9-bgk.c}, as well as a detailed analysis of those techniques.
    To do so, this report is split into several sections corresponding to different iterations of my code.
\end{abstract}

\section{Original Code}

I compiled the original \texttt{d2q9-bgk.c} using the GNU Compiler Collection (GCC) with the following command:
\begin{lstlisting}[language=bash, breaklines=true, breakatwhitespace=true]
gcc -std=c99 -Wall -O3 d2q9-bgk.c -lm -o d2q9-bgk.
\end{lstlisting}

\begin{table}[htbp]
    \begin{center}
    \caption{Execution times of the original code}\label{tab:original}
    \begin{tabular}{l | l} 
        \hline\hline
        Test Case Size&Time (s)\\
        \hline
        $128 \times 128$&\texttt{ 29.16}\\
        $128 \times 256$&\texttt{ 58.71}\\
        $256 \times 256$&\texttt{233.32}\\
        $1024 \times 1024$&\texttt{980.89}\\
        \hline
      \end{tabular}
    \end{center}
\end{table} 

Figure \ref{tab:original} contains the total time to initialise, compute and collate each of the test cases when running the ELF file produced.
It was important to measure the original code, so that I could quantify the performance improvements of my latter implementations.
I measured each of the total times by taking an average of 10 runs on BlueCrystal Phase 4's (BC4's) compute nodes.
Each of BC4's compute nodes is a Lenovo nx360 M5, which contains two 14-core 2.4 GHz Intel E5-2680 v4 (Broadwell) CPUs and 128 GiB of RAM \cite{bcp4}.
I took an average of multiple runs because of the variation between runs, which exists due to the inconsistent performance of compute nodes.
% Not all compute nodes offer the same performance all of the time, due to differing placement in the data centre, amongst other reasons.
It is important to note that I measured every execution time in this report in an identical manner to ensure my results were fair and unskewed.

\section{Serial Optimisations}

\subsection{Compiler}

\begin{table}[htbp]
  \begin{center}
  \caption{Execution times after compiler changes, and speedup over the original code}\label{tab:compiler_changes}
  \begin{tabular}{l | l l} 
      \hline\hline
      Grid Size&Time (s)&Speedup\\
      \hline
      $128 \times 128$&\texttt{ 22.25}&\texttt{1.31}\\
      $128 \times 256$&\texttt{ 44.42}&\texttt{1.32}\\
      $256 \times 256$&\texttt{176.69}&\texttt{1.33}\\
      $1024 \times 1024$&\texttt{795.41}&\texttt{1.23}\\
      \hline
    \end{tabular}
  \end{center}
\end{table} 

I compiled my serial optimised implementation using the IntelÂ® C Compiler as opposed to GCC, since it provides better optimised code for Intel processors.
Furthermore, I compiled my code with the \texttt{Ofast} flag, which set aggressive options to improve the speed of my program, including \texttt{O3} optimisations and aggressive floating point optimisations \cite{icc}.

\subsection{Loop Fusion and Pointer Swap}

\begin{table}[htbp]
  \begin{center}
  \caption{Execution times after loop fusion and pointer swap, and speedup over the original code}\label{tab:loop_fusion_pointer_swap}
  \begin{tabular}{l | l l} 
      \hline\hline
      Test Case Size&Time (s)&Speedup\\
      \hline
      $128 \times 128$&\texttt{ 19.42}&\texttt{1.50}\\
      $128 \times 256$&\texttt{ 39.21}&\texttt{1.50}\\
      $256 \times 256$&\texttt{155.64}&\texttt{1.50}\\
      $1024 \times 1024$&\texttt{635.61}&\texttt{1.54}\\
      \hline
    \end{tabular}
  \end{center}
\end{table} 

LBM is a memory bound problem.
As a result of this, there was a significant opportunity to optimise \texttt{d2q9-bgk.c} by decreasing the number of memory accesses.
One method I utilised to accomplish this was loop fusion.
In the original code, the entire grid was iterated over in four sequential procedures within each timestep: \texttt{propagate}, \texttt{rebound}, \texttt{collision} and \texttt{av\_velocity}.
By fusing the four loops in these procedures into one, I was able to drastically decrease the number of memory accesses, thereby improving the performance of my program.

Implementing loop fusion offered another significant opportunity to eliminate redundant memory accesses.
The original code had a significant quantity of value copying between the \texttt{cells} and \texttt{tmp\_cells} arrays.
I was able to eliminate this by writing all new values of cells to a \texttt{cells\_new} array, and simply swapping the pointers of \texttt{cells\_new} and \texttt{cells} at the end of each timestep.
Furthermore, I eliminated the \texttt{tmp\_cells} array entirely.

\subsection{Arithmetic Improvements}

\begin{table}[htbp]
  \begin{center}
  \caption{Execution times after arithmetic improvements, and speedup over the original code}\label{tab:arithmetic_improvements}
  \begin{tabular}{l | l l} 
      \hline\hline
      Test Case Size&Time (s)&Speedup\\
      \hline
      $128 \times 128$&\texttt{ 19.10}&\texttt{1.53}\\
      $128 \times 256$&\texttt{ 38.49}&\texttt{1.53}\\
      $256 \times 256$&\texttt{153.39}&\texttt{1.52}\\
      $1024 \times 1024$&\texttt{621.52}&\texttt{1.58}\\
      \hline
    \end{tabular}
  \end{center}
\end{table} 

Despite the compiler being able to partially optimise the arithmetic within each timestep without making any changes to the code, there were still some manual improvements that I made to improve the performance of the program.
Division is a very slow arithmetic operation relative to multiplication.
Therefore, to eliminate a large number of unnecessary division operations I precalculated several values including:
\[
    \frac{1}{c^2} = 3\qquad
    \frac{1}{2c^2} = 1.5\qquad
    \frac{1}{2c^4} = 4.5
\]
where $c$ is the speed of sound.
Additionally, I noticed that the number of cells in the grid that were not obstacles \texttt{tot\_u} was recalculated and then divided by each timestep.
I eliminated this inefficiency by counting number of cells that were not obstacles only once (during the initialisation phase).
I then saved the reciprocol of this value as a parameter \texttt{num\_non\_obstacles\_r}, which I used once per timestep in a multiplicative operation to compute the average velocity.

\subsection{Vectorization}

Vectorization is the process of converting a scalar implementation to a vector implementation, which enables the compiler to make use of additional registers to perform multiple operations in a single instruction \cite{vectorization}.

\begin{table}[htbp]
    \begin{center}
    \caption{Execution times after vectorization, and speedup over the original code}\label{tab:vectorized}
    \begin{tabular}{l | l l} 
        \hline\hline
        Test Case Size&Time (s)&Speedup\\
        \hline
        $128 \times 128$&\texttt{  \,\,\,5.79}&\texttt{5.04}\\
        $128 \times 256$&\texttt{ 11.66}&\texttt{5.04}\\
        $256 \times 256$&\texttt{ 40.81}&\texttt{5.72}\\
        $1024 \times 1024$&\texttt{213.53}&\texttt{4.59}\\
        \hline
      \end{tabular}
    \end{center}
\end{table}

\section{Parallelism}

\subsection{OpenMP}

\begin{table}[htbp]
  \begin{center}
  \caption{Execution times after parallelising, and speedup over both the original and vectorized code}\label{tab:parallelised}
  \begin{tabular}{l | l  l  l} 
      \hline\hline
      &&\multicolumn{2}{c}{Speedup}\\
      \cline{3-4}
      Grid Size&Time (s)&Original&Vectorized\\
      \hline
      $128 \times 128$&\texttt{ 0.79}&\texttt{36.91}&\texttt{ 7.33}\\
      $128 \times 256$&\texttt{ 0.91}&\texttt{64.52}&\texttt{12.82}\\
      $256 \times 256$&\texttt{ 2.85}&\texttt{81.87}&\texttt{14.32}\\
      $1024 \times 1024$&\texttt{13.43}&\texttt{73.04}&\texttt{15.90}\\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{Scaling}

\clearpage

\onecolumn{
  \printbibliography
}

\end{document}