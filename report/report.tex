\documentclass[twocolumn, a4paper]{article}
\usepackage[a4paper, left = 2cm, right = 2cm, top = 2cm, bottom = 2cm]{geometry}
\usepackage[style = numeric, sorting = none, urldate = long]{biblatex}
\usepackage{subfig}
\usepackage{pgfplots}
\usepackage[T1]{fontenc}
\usepackage{graphicx}

\graphicspath{{./images/}}
\addbibresource{refs.bib}

\author{
    George Herbert\\
    \texttt{cj19328@bristol.ac.uk}
}

\title{\vspace{-2em}Optimising and Parallelising d2q9-bgk.c}
\begin{document}

\maketitle

\begin{abstract}
    \texttt{d2q9-bgk.c} implements the Lattice Boltzmann method (LBM) to simulate a fluid density on a lattice.
    This report outlines the techniques I utilised to optimise and parallelise \texttt{d2q9-bgk.c} and a detailed analysis of those techniques.
\end{abstract}

\section{Original Code}

I ran the provided \texttt{Makefile} to compile the original \texttt{d2q9-bgk.c} code, which executed the GNU Compiler Collection (GCC) with the \texttt{-std=c99} \texttt{-O3} and \texttt{-lm} options.
Table \ref{tab:original} contains the total time to initialise, compute and collate each test case when running the executable.

\begin{table}[htbp]
    \begin{center}
    \caption{Execution times of the original code}\label{tab:original}
    \begin{tabular}{l | l} 
        \hline\hline
        Grid Size&Time (s)\\
        \hline
        $128 \times 128$&\texttt{ 29.16}\\
        $128 \times 256$&\texttt{ 58.71}\\
        $256 \times 256$&\texttt{233.32}\\
        $1024 \times 1024$&\texttt{980.89}\\
        \hline
      \end{tabular}
    \end{center}
\end{table} 

It was essential to measure the original code to quantify the performance improvements of my latter implementations.
I measured each of the total times by taking an average of five runs on BlueCrystal Phase 4's (BC4's) compute nodes; each of BC4's compute nodes was a Lenovo nx360 M5, which contained two 14-core 2.4 GHz Intel E5-2680 v4 (Broadwell) CPUs and 128 GiB of RAM \cite{bcp4}.
I took an average of multiple runs because of the variation between runs, which existed due to the inconsistent performance of compute nodes.
% Not all compute nodes offer the same performance all of the time, due to differing placement in the data centre, amongst other reasons.

\section{Serial Optimisations}

% I initially implemented a collection of serial optimisations to improve the performance of \texttt{d2q9-bgk.c}.

\subsection{Compiler}

The first improvement I implemented was compiling with the Intel C Compiler Classic (ICC) instead of GCC since it produced an executable better optimised for BC4's Intel compute nodes.
Furthermore, I compiled my code with the \texttt{-Ofast} option, which set aggressive options to improve the speed of my program, including \texttt{-O3} optimisations and aggressive floating-point optimisations \cite{icc}.
These changes to the compilation process provided a good performance boost, as shown in Table \ref{tab:compiler_changes}.

\begin{table}[htbp]
  \begin{center}
  \caption{Execution times after compilation changes, and speedup over the original code}\label{tab:compiler_changes}
  \begin{tabular}{l | l l} 
      \hline\hline
      Grid Size&Time (s)&Speedup\\
      \hline
      $128 \times 128$&\texttt{ 22.25}&\texttt{1.31}\\
      $128 \times 256$&\texttt{ 44.42}&\texttt{1.32}\\
      $256 \times 256$&\texttt{176.69}&\texttt{1.33}\\
      $1024 \times 1024$&\texttt{795.41}&\texttt{1.23}\\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{Loop Fusion and Pointer Swap}

Once I had improved the compilation process, I used the Intel Advisor tool to generate a Roofline chart, as shown in Figure \ref{fig:roofline_compiler}.

\begin{figure}[htbp]
\includegraphics[width=\linewidth]{roofline_compiler.png}
\caption{Screenshot of the Intel Advisor Roofline Analysis for my implementation following compilation changes, run on the $1024\times1024$ test case}\label{fig:roofline_compiler}
\end{figure}

I identified several attributes of my program and several key areas to focus my optimisations.
Firstly, for larger grid sizes especially, the LBM implementation in \texttt{d2q9-bgk.c} was a memory bandwidth bound problem.
\texttt{d2q9-bgk.c} achieved an average arithmetic intensity of 0.25 FLOP/byte and performance of 3.44 GFLOPS for the $1024 \times 1024$ test case.

From this, I identified a significant opportunity to optimise \texttt{d2q9-bgk.c} by decreasing the number of memory accesses.
Decreasing the quantity of bytes accessed would simultaneously increase the performance of the implementation whilst also increasing the arithmetic intensity, which would have the additional benefit of increasing the performance bound for my latter implementations.

One method I utilised to accomplish this was loop fusion.
In the original code, the entire grid was iterated over in four sequential procedures within each timestep: \texttt{propagate}, \texttt{rebound}, \texttt{collision} and \texttt{av\_velocity}.
By absorbing these four procedures into the \texttt{timestep} procedure and fusing the four loops, I drastically decreased the number of memory accesses, thereby improving the performance of my program.

Implementing loop fusion offered another significant opportunity to eliminate redundant memory accesses.
The original code contained substantial value copying between the \texttt{cells} and \texttt{tmp\_cells} arrays.
I was able to eliminate this by writing all new values of cells to a \texttt{cells\_new} array, and simply swapping the pointers of \texttt{cells\_new} and \texttt{cells} at the end of each timestep.
I removed the \texttt{tmp\_cells} array entirely.

Table \ref{tab:loop_fusion_pointer_swap} displays the overall performance improvements.
Furthermore, as a result of these optimisations, the arithmetic intensity increased to 0.29 FLOP/byte and the performance increased to 3.54 GFLOPS for the $1024 \times 1024$ test case.

\begin{table}[htbp]
  \begin{center}
  \caption{Execution times after loop fusion and pointer swap, and speedup over the original code}\label{tab:loop_fusion_pointer_swap}
  \begin{tabular}{l | l l} 
      \hline\hline
      Grid Size&Time (s)&Speedup\\
      \hline
      $128 \times 128$&\texttt{ 19.42}&\texttt{1.50}\\
      $128 \times 256$&\texttt{ 39.21}&\texttt{1.50}\\
      $256 \times 256$&\texttt{155.64}&\texttt{1.50}\\
      $1024 \times 1024$&\texttt{635.61}&\texttt{1.54}\\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{Arithmetic Improvements}

Despite the compiler being able to partially optimise the arithmetic within each timestep without making any changes to the code, there were still some manual improvements that I made to improve the program's performance.
Division operations take considerably more time to execute than other basic arithmetic operations, such as multiplication.
Therefore, to eliminate a large number of unnecessary division operations, I precalculated several values, including:
\[
    \frac{1}{c^2} = 3\qquad
    \frac{1}{2c^2} = 1.5\qquad
    \frac{1}{2c^4} = 4.5
\]
where $c$ is the speed of sound.
Additionally, I noticed that the number of cells in the grid that were not obstacles \texttt{tot\_u} was recalculated and then divided by each timestep.
I eliminated this inefficiency by counting the number of cells that were not obstacles only once (during the initialisation phase).
I then saved the reciprocal of this value as a parameter \texttt{num\_non\_obstacles\_r}, which I used once per timestep in a multiplicative operation to compute the average velocity.

These arithmetic improvements provided only a slight boost to performance compared to the prior implementation, as shown in Table \ref{tab:arithmetic_improvements}.
However, this was unsurprising since these operations were only a small percentage of the operations in my program.

\begin{table}[htbp]
  \begin{center}
  \caption{Execution times after arithmetic improvements, and speedup over the original code}\label{tab:arithmetic_improvements}
  \begin{tabular}{l | l l} 
      \hline\hline
      Grid Size&Time (s)&Speedup\\
      \hline
      $128 \times 128$&\texttt{ 19.10}&\texttt{1.53}\\
      $128 \times 256$&\texttt{ 38.49}&\texttt{1.53}\\
      $256 \times 256$&\texttt{153.39}&\texttt{1.52}\\
      $1024 \times 1024$&\texttt{621.52}&\texttt{1.58}\\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{Vectorization}

Vectorization is the process of converting a scalar implementation to a vector implementation, which enables the compiler to use additional registers to perform multiple operations in a single instruction \cite{vectorization}.
I utilised several techniques to enforce single-instruction-multiple-data (SIMD) vectorization of the inner loop within each timestep.

Firstly, I converted the \texttt{t\_speed} structure holding cell speeds from an array of structures (AoS) to a structure of arrays (SoA).
In the SoA format, the \texttt{t\_speed} structure contained nine pointers, each to an individual array of floats, represented the grid.
Each array of floats contained the values of one vector for each cell within the grid.
The SoA format greatly suited vectorisation of the inner loop since it kept memory accesses contiguous over structure instances \cite{soa}.

Having altered the data layout to suit vectorization, I utilised several other techniques to enforce vectorization.
I implemented the \texttt{\#pragma omp simd} pragma to vectorise the inner loop within each timestep.
This pragma indicated to the compiler to utilise SIMD instructions to execute operations within the inner loop on multiple data elements in a single instruction.
Since I was compiling my code with the \texttt{Ofast} optimisation level, the \texttt{qopenmp-simd} option was already enabled to utilise OpenMP SIMD compilation \cite{icc}.
Furthermore, I utilised the \texttt{reduction(+:tot\_u)} clause to ensure the \texttt{tot\_u} variable contained the correct value at the loop's termination.

I compiled my code with the \texttt{-restrict} option and used the \texttt{restrict} keyword to define each of the nine pointers in \texttt{t\_speed}.
The \texttt{restrict} keyword asserted that the memory referenced by these pointers was not aliased.
Overall, this reduced the execution time by preventing the compiler from performing a runtime check for aliasing.

Processors efficiently move data located on specific byte boundaries by the nature of their design, and compilers can perform optimisations when data access is known to be aligned by 64 bytes \cite{alignment}.
To align the \texttt{cells}, \texttt{cells\_new} and \texttt{obstacles} variables, I replaced calls to the \texttt{malloc} and \texttt{free} procedures with the alignment specific replacements: \texttt{\_mm\_malloc} and \texttt{\_mm\_free}, respectively.
I used the \texttt{\_\_assume\_\_aligned} procedure and  the statement \texttt{\_\_assume(params.nx \% 16 == 0)} to inform the compiler that the dynamically allocated variables were aligned.
Doing so prevented the compiler from generating conservative code, which would have been detrimental to the speed of my implementation.

Once I had utilised these techniques to enforce efficient vectorization of the inner loop, I compiled \texttt{d2q9-bgk.c} with the \texttt{-xAVX2} option to direct the compiler to optimise for Intel processors that support Advanced Vector Extensions 2 (AVX2) (which BC4's compute nodes do) \cite{lenovo}.

Vectorization provided the most considerable improvement to speedup of any optimisation that I had implemented to this point, as shown in Table \ref{tab:vectorized}.
Furthermore, my implementation achieved an arithmetic intensity of 0.43 FLOP/byte and performance of 10.14 GFLOPS when run on the $1024\times1024$ test case.
% This significant improvement was unsurprising since AVX2 could perform simultaneous operations on up to eight single-precision floating-point numbers.

\begin{table}[htbp]
  \begin{center}
  \caption{Execution times after vectorization, and speedup over the original code}\label{tab:vectorized}
  \begin{tabular}{l | l l} 
      \hline\hline
      Grid Size&Time (s)&Speedup\\
      \hline
      $128 \times 128$&\texttt{  \,\,\,5.77}&\texttt{5.05}\\
      $128 \times 256$&\texttt{ 11.57}&\texttt{5.07}\\
      $256 \times 256$&\texttt{ 41.55}&\texttt{5.62}\\
      $1024 \times 1024$&\texttt{215.52}&\texttt{4.55}\\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\section{Parallelism}

% Overall, my serial optimisations (including vectorization) achieved approximately five times speedup over the original implementation.
% However, there was an immense potential for performance improvements by parallelising \texttt{d2q9-bgk.c}. 

\subsection{OpenMP}

OpenMP implements parallelism by launching a set of threads that execute portions of code concurrently \cite{openmp_cornell}.
I utilised OpenMP's \texttt{\#pragma omp parallel for} pragma to direct the compiler to parallelise the outer loop in the \texttt{timestep} procedure.
Furthermore, I compiled my code with the \texttt{qopenmp} option, which enabled the parallelizer to generate multithreaded code based on OpenMP directives.
Since the \texttt{tot\_u} variable needed to contain the total velocities of each cell, I used the clause \texttt{reduction(+:tot\_u)} to prevent race conditions; the reduction clause informed the compiler to create a copy of the \texttt{tot\_u} variable for each thread (initialised to zero), and to sum the local results when the outer loop terminated.

Table \ref{tab:parallelised} displays the execution times for my parallel implementation (run with 28 threads), and speedup over both the original and vectorized code.

\begin{table}[htbp]
  \begin{center}
  \caption{Execution times after parallelising (run with 28 threads), and speedup over both the original and vectorized code}\label{tab:parallelised}
  \begin{tabular}{l | l  l  l} 
      \hline\hline
      &&\multicolumn{2}{c}{Speedup}\\
      \cline{3-4}
      Grid Size&Time (s)&Original&Vectorized\\
      \hline
      $128 \times 128$&\texttt{ 1.14}&\texttt{}&\texttt{}\\
      $128 \times 256$&\texttt{ 1.35}&\texttt{}&\texttt{}\\
      $256 \times 256$&\texttt{ 3.33}&\texttt{}&\texttt{}\\
      $1024 \times 1024$&\texttt{14.38}&\texttt{}&\texttt{}\\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{Non-Uniform Memory Access (NUMA)}

NUMA is a computer memory design in which memory access time depends on the memory location relative to the processor \cite{numa}.
Memory is allocated to the closest NUMA region to the thread that first touches the data \cite{numa_bristol}.
Since BC4's compute nodes contain two sockets, the memory access time for a given thread primarily depends on whether the memory is connected to the socket the thread resides in or not.
As a result of this, I parallelised the initialisation loops for \texttt{cells} and \texttt{obstacles} to ensure that each thread touched the same data in both the \texttt{initialise} and \texttt{compute} procedures.
Furthermore, I set the environment variables \texttt{OMP\_PROC\_BIND=true} and \texttt{OMP\_PLACES=cores} to prevent threads from moving cores.

Table \ref{tab:numa} contains the updated execution times for my final NUMA-aware implementation.

\begin{table}[htbp]
  \begin{center}
  \caption{Execution times after writing NUMA-aware implementation (run with 28 threads), and speedup over both the original and vectorized code}\label{tab:numa}
  \begin{tabular}{l | l  l  l} 
      \hline\hline
      &&\multicolumn{2}{c}{Speedup}\\
      \cline{3-4}
      Grid Size&Time (s)&Original&Vectorized\\
      \hline
      $128 \times 128$&\texttt{ 0.72}&\texttt{}&\texttt{}\\
      $128 \times 256$&\texttt{ 0.80}&\texttt{}&\texttt{}\\
      $256 \times 256$&\texttt{ 2.47}&\texttt{}&\texttt{}\\
      $1024 \times 1024$&\texttt{12.19}&\texttt{}&\texttt{}\\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{Scaling}

I ran my final, NUMA-aware implementation on one to 28 threads to gain an insight into how my implementation scaled.
I calculated the speedup that subsequent threads provided over a single thread implementation.
Figure \ref{fig:scaling} displays the resultunt speedup curves.

\begin{figure}[htpb]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={Number of Threads},
      ylabel={Speedup},
      xmin = 1, xmax = 28,
      ymin = 1,
      % xtick={0, 5, 10, 15, 20, 25},
      % ytick={0,20,40,60,80,100,120},
      legend pos=north west,
      ymajorgrids=true,
      grid style=dashed,
    ]
    \addlegendentry{$128\times128$}
    \addplot[color = orange, mark = x]coordinates{
      (1, 1)
      (2, 1.770147781)
      (3, 2.479203736)
      (4, 3.090127044)
      (5, 3.495898299)
      (6, 3.969781807)
      (7, 4.449517419)
      (8, 4.885230537)
      (9, 5.105636521)
      (10, 5.495622472)
      (11, 5.832498588)
      (12, 6.033959916)
      (13, 6.511971973)
      (14, 6.581786272)
      (15, 7.08579377)
      (16, 7.224322206)
      (17, 7.143221767)
      (18, 7.27187543)
      (19, 7.559142588)
      (20, 7.752570216)
      (21, 7.795101063)
      (22, 7.970592183)
      (23, 8.058757757)
      (24, 8.411400223)
      (25, 8.309209659)
      (26, 8.448042356)
      (27, 8.873950316)
      (28, 8.729508452)
    };
    \addlegendentry{$128\times256$}
    \addplot[color = blue, mark = x]coordinates{
      (1, 1)
      (2, 1.918812078)
      (3, 2.722691991)
      (4, 3.620768356)
      (5, 4.129091999)
      (6, 4.720896994)
      (7, 5.457129627)
      (8, 5.890235496)
      (9, 6.398711601)
      (10,6.861778085)
      (11,7.679664463)
      (12,7.892142111)
      (13,8.225546911)
      (14,9.002082643)
      (15,9.431524198)
      (16,10.01307455)
      (17,9.424918639)
      (18,9.751774957)
      (19,10.9890841)
      (20,10.83510235)
      (21,11.67306753)
      (22,11.04704628)
      (23,11.80091066)
      (24,11.73689961)
      (25,12.3300502)
      (26,12.27839835)
      (27,14.65659506)
      (28,14.92261783)
    };
    \addlegendentry{$256\times256$}
    \addplot[color = green, mark = x]coordinates{
      (1, 1)
      (2, 1.938375582)
      (3, 2.771151595)
      (4, 3.675722466)
      (5, 4.299722521)
      (6, 5.052146164)
      (7, 5.643422171)
      (8, 6.316100162)
      (9, 7.04401679)
      (10, 7.429468926)
      (11, 8.075812314)
      (12, 8.879274603)
      (13, 9.352559102)
      (14, 10.00947825)
      (15, 10.57828753)
      (16, 11.29099838)
      (17, 11.34786525)
      (18, 11.9302309)
      (19, 12.54826914)
      (20, 12.94577227)
      (21, 13.4034522)
      (22, 13.76922924)
      (23, 14.16603038)
      (24, 15.03635571)
      (25, 15.85516854)
      (26, 16.38362619)
      (27, 16.95314186)
      (28, 17.18947447)

    };
    \addlegendentry{$1024\times1024$}
    \addplot[color = red, mark = x]coordinates{
      (1, 1)
      (2, 2.100760194)
      (3, 2.870122668)
      (4, 4.071422641)
      (5, 4.614599066)
      (6, 5.652837154)
      (7, 6.291146923)
      (8, 7.383068123)
      (9, 7.881869892)
      (10, 8.977180241)
      (11, 9.452161408)
      (12, 10.41116263)
      (13, 11.06645567)
      (14, 11.91753689)
      (15, 12.0840354)
      (16, 13.08055546)
      (17, 14.05218119)
      (18, 14.89108656)
      (19, 15.38824382)
      (20, 15.51458921)
      (21, 15.93993521)
      (22, 16.75254912)
      (23, 15.52257546)
      (24, 16.33948228)
      (25, 17.84378604)
      (26, 18.02294074)
      (27, 17.16198512)
      (28, 18.21858148)
    };
    \end{axis}
  \end{tikzpicture}
  \caption{Speedup curves for my NUMA-aware implementation}\label{fig:scaling}
\end{figure}

In general, my implementation initially scaled well for each grid size, but the speedup acquired from each subsequent declines---this is known as a sublinear plateau.
There are some exceptions to this trend.
Most evidently, there is a significant jump in speedup for the $128\times256$ test case at 27 cores.
Whilst this could be due to noise, it is also potentially due to the program splitting the grid sufficiently small to fit on each core's L1 or L2 cache.

Notably, the amount of speedup provided by each subsequent core is inversely proportional to the test case size.
In other words, larger grid sizes benefit more from a multithreaded implementation than smaller grid sizes.

\subsection{Comparison to Serial Optimised}

I used the Intel Advisor tool to analyse the performance of my final implementation, as shown in Figure \ref{fig:roofline_numa}.
On the $1024\times1024$ test case, an arithmetic intensity of 0.43 FLOP/byte and performance of 168.35 GFLOPS was achieved.

\begin{figure}[htbp]
  \includegraphics[width=\linewidth]{roofline_numa.png}
  \caption{Screenshot of the Intel Advisor Roofline Analysis for my NUMA-aware implementation, run with 28 threads on the $1024\times1024$ test case}\label{fig:roofline_numa}
\end{figure}

Compared to my vectorized implementation, the arithmetic intensity was identical, whereas the performance increased by a factor of 16.60.
This was primarily due to two reasons.
Firstly, by running across multiple threads on multiple cores, sections of the grid could be computed in parallel.
Secondly, inspecting the Intel Advisor memory metrics for the $1024\times1024$ grid size highlighted that the parallel program did not have to interact with the DRAM, which had a lower bandwidth, as often.
For example, in the \texttt{timestep} loop in the vectorized implementation, 5130.56 GB and 2306.03 GB of data was passed through the L1 cache and DRAM, respectively.
Whereas in the NUMA-aware implementation 5777.32 GB and 1447.21 GB of data was passed through the L1 cache and DRAM, respectively. 

\section{LBM in Go}

There are many other languages, APIs and libraries that can be utilised to implement LBM, each with their own advantages and disadvantages.

Unlike C, Go is a language with concurrency built into its core design.
Consequently, parallelism can be effortlessly achieved in Go, without requiring knowlege of the underlying system, or APIs such as OpenMP.
As a final, interesting experiment, I produced an implementation of LBM in Go to identify whether a comparable amount of performance could be achieved.

In experiments, the Go implementation was between seven and sixteen times slower.
Specifically, it achieved a time of 7.66 seconds, 13.10 seconds, 41.83 seconds and 103.02 seconds on the increasingly large test cases.
Upon inspection, there were likely several reasons for the comparatively poor performance.
Firstly, Go's compiler did not perform many optimisations.
Compiling with GCCGO or GOLLVM may have produced a better optimised executable; however, these required a complicated installation process, which was directly contradictory to the desired simplicity for the experiment.
Secondly, and perhaps most prominently, was the lack of vectorization.
Vectorizing code in Go requires the use of external libraries or assembly, which similarly conflicted with the desired simplicity.

In conclusion, a comparable amount of performance could not be achieved using Go's core features.

\clearpage

\onecolumn{
  \printbibliography
}

\end{document}