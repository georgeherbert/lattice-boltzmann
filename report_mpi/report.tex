\documentclass[twocolumn, a4paper]{article}
\usepackage[a4paper, left = 1.75cm, right = 1.75cm, top = 1.75cm, bottom = 1.75cm]{geometry}
\usepackage[style = numeric, sorting = none]{biblatex}
\usepackage{pgfplots}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{arydshln}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}

\graphicspath{{./images/}}
\addbibresource{refs.bib}

\cleanlookdateon

\renewcommand*{\bibfont}{\footnotesize}

\author{
  George Herbert\\
  \texttt{cj19328@bristol.ac.uk}
}

\title{\vspace{-2em}Parallelising d2q9-bgk.c with MPI}

\begin{document}

\maketitle

\begin{abstract}
  \texttt{d2q9-bgk.c} implements the Lattice Boltzmann method (LBM) to simulate a fluid density on a lattice.
  This report analyses the techniques I utilised to parallelise \texttt{d2q9-bgk.c} with MPI, and port \texttt{d2q9-bgk.c} to a GPU with OpenCL.
\end{abstract}

\section{Single Program, Multiple Data}

\subsection{Hypothesis}

I previously achieved a substantial performance improvement producing a multithreaded implementation of \texttt{d2q9-bgk.c} with OpenMP.
However, since OpenMP was built for shared-memory parallelism, my implementation could not utilise more than one node of BC4, which was a considerable restriction.

Single program, multiple data (SPMD) is a form of parallelism in which independent processes run the same program.
Message Passing Interface (MPI) is a specification for a library interface for passing messages between processes.
Therefore, I hypothessied that a parallel implementation of \texttt{d2q9-bgk.c} that ran on multiple processes across multiple nodes---with MPI being used for interprocess communication---would provide an even more substantial performance improvement.

I opted to use the implementation of my program prior to enforcing single instruction, multiple data (SIMD) vectorization as a starting point.
I was uncertain the changes I previously implemented to enforce SIMD vectorization would provide a performance benefit to my MPI implementation.

\subsection{Compiler}

I used the \texttt{mpiicc} wrapper script, which compiled my program with the Intel C Compiler Classic (version 19.1.3.304) and set up the include and library paths for the Intel MPI library.

\subsection{Load Balancing}

I had to explicitly assign different sections of the grid to different processes.
It was crucial for the distribution to be adequately balanced to minimise the amount of time processes' spent blocked.

Since the \texttt{cells} grid was stored in row-major order, I split the grid horizontally between processes to take advantage of memory locality.
I created a procedure \texttt{allocate\_rows} to balance the load; the procedure assigned each process at least $\lfloor\frac{y}{n}\rfloor$ consecutive rows, with the first $y - \lfloor\frac{y}{n}\rfloor n$ processes each assigned an additional consecutive row, where $y$ was the number of rows and $n$ the number of processes.
Additionally, since updating the value of a given cell required the values of all adjacent cells, each process contained two additional rows reserved for cells in the top and bottom rows of the preceding and succeeding ranks, respectively.
Figure \ref{fig:rows} displays an example allocation for a grid with five rows, split between two processes; the rows allocated to a specific process are highlighted in green, with additional rows required to correctly updated the edge rows highlighted in red.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.75\linewidth]{rows.png}
  \caption{Row allocation example with five rows and two processes}\label{fig:rows}
\end{figure}

I decided against splitting on a sub-row level to avoid unnecessarily increasing the complexity of my program and incurring an additional computational overhead.

\subsection{Halo Exchange}

Since processes are assigned their own virtual memory region, I had to explicitly send the contents of edge rows to neighbouring ranks at the conclusion of each timestep.
To do so, I created a \texttt{halo\_exchange} procedure.
The procedure copied the bottom-most row allocated to each process into the \texttt{send\_row\_buffer} array.
I used the \texttt{MPI\_Sendrecv} procedure to send this buffer to the \texttt{receive\_row\_buffer} of the preceding rank.
The values in the \texttt{receive\_row\_buffer} were then copied into the top additional row.
The same process was then repeated for the top-most row, which was sent to the succeeding rank.

\subsection{Collating}

I created a \texttt{collate} procedure to be executed once all iterations of the \texttt{timestep} procedure were complete.
The procedure had two purposes.
The first purpose was to transfer the the final state of the cells allocated to each process to the master process (i.e. rank zero).
The second purpose was to transfer the partial average velocity values to the master process, and use these values to calculate the correct average velocity at each timestep.

I used the \texttt{MPI\_Send} procedure to send the final state of the cells allocated to each process to the master process.
The master process received these values by executing the \texttt{MPI\_Recv} procedure once for each process.

I also used the \texttt{MPI\_Send} procedure to send the partial average velocity values held by each process to the master process.
The master process received these arrays of values consecutively from each process using the \texttt{MPI\_Recv} procedure, and summed them into a global average velocities array.
Once the arrays had been summed, the \texttt{colate} procedure multiplied each element by \texttt{params.num\_non\_obstacles\_r} in the master process to calculate the correct average velocity at each timestep.

\subsection{Results}

Table \ref{tab:mpi} displays the results of my MPI implementation.
Each time was an average of three runs on BlueCrystal Phase 4 (BC4) compute nodes; which were each a Lenovo nx360 M5, which contained two 14-core 2.4 GHz Intel E5-2680 v4 (Broadwell) CPUs and 128 GiB of RAM \cite{bcp4}.
My MPI implementation provided a substantial speedup over both my prior implementation (before enforcing SIMD vectorization), as well as my OpenMP implementation.
The speedup is primarily achieved by utilising additional cores to execute sections of the \texttt{timestep} procedure in parallel.

\begin{table}[htbp]
  \begin{center}
  \caption{Execution times with the 112 process MPI implementation and speedup over both the prior and 28 thread OpenMP implementation}\label{tab:mpi}
  \begin{tabular}[t]{l | l  l  l} 
      \hline\hline
      &&\multicolumn{2}{c}{Speedup}\\
      \cline{3-4}
      Grid Size&Time (s)&Prior&OpenMP\\
      \hline
      $128 \times 128$&\texttt{}&\texttt{}&\texttt{}\\
      $128 \times 256$&\texttt{}&\texttt{}&\texttt{}\\
      $256 \times 256$&\texttt{}&\texttt{}&\texttt{}\\
      $1024 \times 1024$&\texttt{}&\texttt{}&\texttt{}\\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\section{Optimisations}

\subsection{Vectorization}

I hypothesised that SIMD vectorization of the inner loop would drastically improve the performance of my MPI implementation, as it did with my serial optimised implementation previously.
Therefore, I made the same changes as I did with my serial optimised implementation, including converting the cells' data from an array of structures (AoS) to a structure of arrays (SoA) format.
Implementing the SoA format meant that the \texttt{halo\_exchange} procedure had to be altered since the \texttt{MPI\_Sendrecv} procedure required the address of a single buffer as input.

I could have implemented nine separate \texttt{MPI\_Sendrecv} procedures---one for each speed.
However, I instead decided to copy each of the nine speeds into a single array, followed by a single call to the \texttt{MPI\_Sendrecv}.
I selected this route, since each call to the \texttt{MPI\_Sendrecv} procedure has an associated overhead.
However, this is why I did not enforce SIMD vectorization in my prior implementation, since I could not be certain the performance benefit gained would outwigh the drawback to performance that arises from copying the nine speeds into a single array.

Table \ref{tab:simd} displays the execution times and speedup after enforcing SIMD vectorization.
Enforcing SIMD vectorization provided a substantial speedup.

\begin{table}[htbp]
  \begin{center}
  \caption{Execution times after enforcing SIMD vectorization and speedup over the prior implementation}\label{tab:simd}
  \begin{tabular}[t]{l | l l} 
      \hline\hline
      Grid Size&Time (s)&Speedup\\
      \hline
      $128 \times 128$&\texttt{}&\texttt{}\\
      $128 \times 256$&\texttt{}&\texttt{}\\
      $256 \times 256$&\texttt{}&\texttt{}\\
      $1024 \times 1024$&\texttt{}&\texttt{}\\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{Average Velocities Reduction}

I hypothesised I could use the \texttt{MPI\_Reduce} procedure to reduce the collation time of my program.
Table \ref{tab:reduction} contains the collation times for this implementation, and speedup over the prior implementation.
Using \texttt{MPI\_Reduce} led to a significant reduction in collation time.
However, this did not make a large difference to the overall execution time due to the collation time being so short compared to the compute time.
I would expect a more significant impact on the overall exeuction time for inputs containing more iterations.

\begin{table}[htbp]
  \begin{center}
  \caption{Collate times with the reduction and speedup over the prior implementation}\label{tab:reduction}
  \begin{tabular}[t]{l | l l} 
      \hline\hline
      Grid Size&Time (s)&Speedup\\
      \hline
      $128 \times 128$&\texttt{0.0016}&\texttt{4.54}\\
      $128 \times 256$&\texttt{0.0029}&\texttt{2.93}\\
      $256 \times 256$&\texttt{0.0049}&\texttt{2.83}\\
      $1024 \times 1024$&\texttt{0.0420}&\texttt{1.08}\\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\section{Experiments}

\subsection{Hybrid MPI and OpenMP}

I sought to investigate whether a hybrid implementation would execute faster than my prior implementation.
Therefore, I produced an implementation of \texttt{d2q9-bgk.c} that ran with eight processes---one per socket across four nodes---communicating via MPI, with each process branching fourteen threads at specific points in the program.
These specific points were identical to that in my OpenMP implementation: the outer loop in the \texttt{timestep} procedure, and the initialisation loops for the \texttt{obstacles} and \texttt{cells} arrays.

I tested my program with the four grids provided, as well as three additional grids that I produced.
Table \ref{tab:hybrid} displays the results of my experiment.
The hybrid implementation was slower with the smaller grids that were provided, but faster with the larger grids that I produced myself.
This was because, with the smaller grids, the overhead introduced by creating and synchronising OpenMP threads each timestep outweighed the performance benefit of fewer halo exchanges.
However, the hybrid implementation excelled with the larger grids, since the overhead introduced by OpenMP was near identical, but the time spent performing halo exchanges increased dramatically. 

\begin{table}[htbp]
  \begin{center}
  \caption{Execution times with the hybrid implementation and speedup over the prior implementation}\label{tab:hybrid}
  \begin{tabular}[t]{l | l l} 
      \hline\hline
      Grid Size&Time (s)&Speedup\\
      \hline
      $128 \times 128$&\texttt{ \space0.66}&\texttt{}\\
      $128 \times 256$&\texttt{ \space0.73}&\texttt{}\\
      $256 \times 256$&\texttt{ \space1.93}&\texttt{}\\
      $1024 \times 1024$&\texttt{ \space3.19}&\texttt{}\\
      \hdashline
      $4096 \times 1024$&\texttt{ 17.18}&\texttt{1.32}\\
      $8192 \times 2048$&\texttt{ 91.74}&\texttt{1.04}\\
      $16384 \times 4096$&\texttt{337.16}&\texttt{1.01}\\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{OpenMP vs MPI}

\begin{figure}[htpb]
  \centering
  \resizebox{\columnwidth}{!}{
    \begin{tikzpicture}
      \begin{axis}[
        xlabel={Number of Threads},
        ylabel={Speedup},
        xmin = 1, xmax = 28,
        ymin = 1,
        % xtick={0, 5, 10, 15, 20, 25},
        % ytick={0,20,40,60,80,100,120},
        legend pos=north west,
        ymajorgrids=true,
        grid style=dashed,
        minor xtick = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28}
      ]
      \addlegendentry{OpenMP}
      \addplot[color = {rgb:red,31;green,119;blue,180}, mark = x]coordinates{
        (1, 1.0)
        (2, 1.7525169788462858)
      };
      \addlegendentry{MPI}
      \addplot[color = {rgb:red,255;green,127;blue,14}, mark = x]coordinates{
      };
      \end{axis}
    \end{tikzpicture}
  }
  \caption{Speedup curves for my OpenMP and MPI implementation on the $1024\times1024$ grid}\label{fig:scaling_openmp_mpi}
\end{figure}

\subsection{Scaling}

I ran my final MPI implementation from 1--112 processes to analyse how my program scaled.
My program ran on as few nodes as possible, such that each process was assigned to a single core. 
I then calculated the subsequent processes' speedup over a single process implementation.
Figure \ref{fig:scaling} displays the result speedup curves.

\begin{figure}[htpb]
  \centering
  \resizebox{\columnwidth}{!}{
    \begin{tikzpicture}
      \begin{axis}[
        xlabel={Number of Threads},
        ylabel={Speedup},
        xmin = 1, xmax = 112,
        ymin = 1,
        % xtick={0, 5, 10, 15, 20, 25},
        % ytick={0,20,40,60,80,100,120},
        legend pos=north west,
        ymajorgrids=true,
        grid style=dashed,
        minor xtick = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112}
      ]
      \addlegendentry{$128\times128$}
      \addplot[color = {rgb:red,31;green,119;blue,180}, mark = x]coordinates{
        (1, 1)
        (2, 1.922808444)
        (3, 2.708059802)
        (4, 3.464734781)
        (5, 4.142059923)
        (6, 4.670675057)
        (7, 5.211431651)
        (8, 6.049802054)
        (9, 6.284429188)
        (10, 6.924075746)
        (11, 7.410721042)
        (12, 7.966540587)
        (13, 8.439237681)
        (14, 8.543576051)
        (15, 8.554939509)
        (16, 9.77942865)
        (17, 10.06144707)
        (18, 10.08154615)
        (19, 10.82751095)
        (20, 10.89260709)
        (21, 10.90651815)
        (22, 11.78077728)
        (23, 11.8009378)
        (24, 11.97610095)
        (25, 12.13291302)
        (26, 13.18270023)
        (27, 13.18108019)
        (28, 13.08024123)
        (29, 10.51526129)
        (30, 10.58383832)
        (31, 10.35592237)
        (32, 10.14230555)
        (33, 10.50176722)
        (34, 11.10254374)
        (35, 11.20210963)
        (36, 10.13069742)
        (37, 11.63928136)
        (38, 10.35221879)
        (39, 11.72158548)
        (40, 10.61610358)
        (41, 11.58868031)
        (42, 11.51831083)
        (43, 12.72382438)
        (44, 11.37353226)
        (45, 12.70257103)
        (46, 9.241521877)
        (47, 12.79212232)
        (48, 10.31398423)
        (49, 12.73550096)
        (50, 10.87232078)
        (51, 10.44664227)
        (52, 10.49156307)
        (53, 12.80608538)
        (54, 10.06783665)
        (55, 12.72016191)
        (56, 10.01782704)
        (57, 9.811491369)
        (58, 12.93781849)
        (59, 12.29166551)
        (60, 10.69113173)
        (61, 12.90085142)
        (62, 11.5576171)
        (63, 12.12750043)
        (64, 14.38226654)
        (65, 13.19426521)
        (66, 11.49843923)
        (67, 14.22560451)
        (68, 13.22092837)
        (69, 13.0607442)
        (70, 14.23689233)
        (71, 14.0492829)
        (72, 10.58496152)
        (73, 12.63788173)
        (74, 11.34458411)
        (75, 9.048344226)
        (76, 13.95694552)
        (77, 14.20703059)
        (78, 10.64631358)
        (79, 14.15623475)
        (80, 13.54871693)
        (81, 10.66763395)
        (82, 14.46324628)
        (83, 12.80130253)
        (84, 9.053077377)
        (85, 14.28261536)
        (86, 13.16863695)
        (87, 13.18328092)
        (88, 11.84068965)
        (89, 14.27750429)
        (90, 14.05330958)
        (91, 13.16981575)
        (92, 11.72223637)
        (93, 14.04375416)
        (94, 13.24823774)
        (95, 10.70439261)
        (96, 13.35667621)
        (97, 14.2305021)
        (98, 12.59951023)
        (99, 11.54477978)
        (100, 9.726436232)
        (101, 14.16020099)
        (102, 13.58441421)
        (103, 9.940775763)
        (104, 12.61740837)
        (105, 14.60476445)
        (106, 14.41749489)
        (107, 13.5330826)
        (108, 13.33602024)
        (109, 14.45781616)
        (110, 14.46196727)
        (111, 14.37893291)
        (112, 12.55231676)
      };
      \addlegendentry{$128\times256$}
      \addplot[color = {rgb:red,255;green,127;blue,14}, mark = x]coordinates{
        (1, 1)
        (2, 2.178028484)
        (3, 3.196785844)
        (4, 4.164688341)
        (5, 4.945837588)
        (6, 5.700338725)
        (7, 6.459853863)
        (8, 7.140151441)
        (9, 7.794156638)
        (10, 8.471097779)
        (11, 9.092806151)
        (12, 9.888069146)
        (13, 10.735835)
        (14, 11.13261876)
        (15, 11.86662053)
        (16, 12.91084774)
        (17, 12.48573073)
        (18, 13.76313801)
        (19, 14.47494367)
        (20, 15.44523842)
        (21, 15.53164136)
        (22, 16.30253209)
        (23, 16.45364559)
        (24, 16.42900576)
        (25, 17.50120582)
        (26, 18.66258668)
        (27, 18.75417886)
        (28, 18.69728157)
        (29, 16.85853658)
        (30, 15.05054827)
        (31, 15.71178903)
        (32, 15.05411813)
        (33, 17.95707188)
        (34, 14.30346812)
        (35, 18.08841621)
        (36, 17.00472192)
        (37, 18.082551)
        (38, 18.00636915)
        (39, 19.20980578)
        (40, 14.99893186)
        (41, 19.57893928)
        (42, 18.8635518)
        (43, 21.15086982)
        (44, 17.67812067)
        (45, 21.14896051)
        (46, 15.31124094)
        (47, 21.33161321)
        (48, 19.34701468)
        (49, 19.71519411)
        (50, 16.59716914)
        (51, 21.38950655)
        (52, 18.98927472)
        (53, 23.05531791)
        (54, 21.0485019)
        (55, 23.03724604)
        (56, 20.4488261)
        (57, 16.05517262)
        (58, 22.94505778)
        (59, 22.0651348)
        (60, 21.94895521)
        (61, 21.22015873)
        (62, 18.0140901)
        (63, 19.89672676)
        (64, 25.28730621)
        (65, 18.76325775)
        (66, 19.81000825)
        (67, 23.35346516)
        (68, 24.0431408)
        (69, 21.3132486)
        (70, 25.28932244)
        (71, 19.88445973)
        (72, 21.55720614)
        (73, 25.36621747)
        (74, 25.20085085)
        (75, 24.18224162)
        (76, 25.44783013)
        (77, 21.97415246)
        (78, 14.86229867)
        (79, 25.20823359)
        (80, 25.44376703)
        (81, 19.99333188)
        (82, 25.29467674)
        (83, 24.24834032)
        (84, 25.16986964)
        (85, 25.49728852)
        (86, 27.67149109)
        (87, 26.16157312)
        (88, 27.82572378)
        (89, 27.90751086)
        (90, 27.82654636)
        (91, 26.48489496)
        (92, 24.75372864)
        (93, 27.9508939)
        (94, 26.41014954)
        (95, 22.85484751)
        (96, 26.26628274)
        (97, 28.21556741)
        (98, 27.73270133)
        (99, 26.49618844)
        (100, 23.61317402)
        (101, 27.82729043)
        (102, 26.40723599)
        (103, 26.65524278)
        (104, 26.4087459)
        (105, 28.08446357)
        (106, 27.97894075)
        (107, 26.65102998)
        (108, 26.26789063)
        (109, 28.17805085)
        (110, 28.07259611)
        (111, 27.39171519)
        (112, 26.56061497)
      };
      \addlegendentry{$256\times256$}
      \addplot[color = {rgb:red,44;green,160;blue,44}, mark = x]coordinates{
        (1, 1)
        (2, 1.945127526)
        (3, 3.139079154)
        (4, 3.985983393)
        (5, 4.822401812)
        (6, 5.511872495)
        (7, 6.390039578)
        (8, 6.941399045)
        (9, 7.648248545)
        (10, 8.284034398)
        (11, 8.795940812)
        (12, 9.562259477)
        (13, 10.03169515)
        (14, 10.26531205)
        (15, 11.33136788)
        (16, 12.22284819)
        (17, 12.45403162)
        (18, 13.18239624)
        (19, 13.27572904)
        (20, 14.59847809)
        (21, 14.58203062)
        (22, 15.67842067)
        (23, 15.87842274)
        (24, 16.86538948)
        (25, 16.94164148)
        (26, 18.0560227)
        (27, 18.35251929)
        (28, 18.43458286)
        (29, 17.1735845)
        (30, 16.9412826)
        (31, 17.4331915)
        (32, 17.74811926)
        (33, 18.53478139)
        (34, 18.15746748)
        (35, 18.40949696)
        (36, 17.99296887)
        (37, 20.34962281)
        (38, 20.38547968)
        (39, 20.00247444)
        (40, 20.02228584)
        (41, 19.67624818)
        (42, 20.22456386)
        (43, 22.35300245)
        (44, 21.05388413)
        (45, 22.31112584)
        (46, 22.12390584)
        (47, 22.20380777)
        (48, 21.14818593)
        (49, 22.33288526)
        (50, 20.74693244)
        (51, 21.03502307)
        (52, 24.05305719)
        (53, 24.62561574)
        (54, 23.04482076)
        (55, 24.61593971)
        (56, 23.37306301)
        (57, 21.60198898)
        (58, 24.49883477)
        (59, 24.76817412)
        (60, 23.84267306)
        (61, 24.8774351)
        (62, 24.73459932)
        (63, 24.53765456)
        (64, 27.16163665)
        (65, 27.56365991)
        (66, 27.22912305)
        (67, 27.7917721)
        (68, 26.5092514)
        (69, 27.63510531)
        (70, 26.04517546)
        (71, 26.13540846)
        (72, 26.35943238)
        (73, 27.42046887)
        (74, 27.83873537)
        (75, 25.5020431)
        (76, 27.69210384)
        (77, 24.25847606)
        (78, 26.27019884)
        (79, 27.64934867)
        (80, 27.42871582)
        (81, 24.91520647)
        (82, 27.8711369)
        (83, 27.86195891)
        (84, 25.26948178)
        (85, 27.85795858)
        (86, 30.78228538)
        (87, 30.86531039)
        (88, 29.51948872)
        (89, 31.51723922)
        (90, 31.42733336)
        (91, 30.28669265)
        (92, 30.36085084)
        (93, 31.41414203)
        (94, 31.09725455)
        (95, 30.76079515)
        (96, 28.47261565)
        (97, 29.97695709)
        (98, 30.87137952)
        (99, 30.90962268)
        (100, 30.42258674)
        (101, 31.43276204)
        (102, 29.50436935)
        (103, 29.53203012)
        (104, 30.86278203)
        (105, 31.34884069)
        (106, 31.08695855)
        (107, 31.28857046)
        (108, 30.40892566)
        (109, 30.13636152)
        (110, 30.2196501)
        (111, 29.55258643)
        (112, 29.19595139)
      };
      \addlegendentry{$1024\times1024$}
      \addplot[color = {rgb:red,214;green,39;blue,40}, mark = x]coordinates{
        (1, 1)
        (2, 2.183346182)
        (3, 2.84296167)
        (4, 4.015639583)
        (5, 4.431608929)
        (6, 5.273048985)
        (7, 6.166801173)
        (8, 6.999372908)
        (9, 7.703520359)
        (10, 7.337916865)
        (11, 8.163524538)
        (12, 9.161177786)
        (13, 10.66639541)
        (14, 11.81972027)
        (15, 11.6473548)
        (16, 13.38231529)
        (17, 12.80253891)
        (18, 14.37527875)
        (19, 14.15641925)
        (20, 15.32393877)
        (21, 14.75532875)
        (22, 15.92105208)
        (23, 14.82346611)
        (24, 16.3980624)
        (25, 15.33596165)
        (26, 16.70138412)
        (27, 15.36831973)
        (28, 16.53543585)
        (29, 24.81193061)
        (30, 25.34543663)
        (31, 26.35953332)
        (32, 26.84491675)
        (33, 27.44101463)
        (34, 29.12136554)
        (35, 30.23160433)
        (36, 36.06444608)
        (37, 38.21006127)
        (38, 40.15678012)
        (39, 39.27706219)
        (40, 40.71757248)
        (41, 43.83255421)
        (42, 43.81462708)
        (43, 45.02816718)
        (44, 45.4528944)
        (45, 45.5342091)
        (46, 44.12775544)
        (47, 45.70433758)
        (48, 45.16468146)
        (49, 50.039844)
        (50, 49.07842726)
        (51, 49.60778642)
        (52, 50.19510353)
        (53, 52.37488435)
        (54, 51.88908592)
        (55, 54.14957816)
        (56, 53.99356924)
        (57, 53.99389435)
        (58, 56.26674119)
        (59, 56.83444992)
        (60, 54.74579413)
        (61, 59.68119116)
        (62, 58.2549271)
        (63, 57.87581148)
        (64, 61.32862571)
        (65, 61.42676501)
        (66, 62.56983789)
        (67, 63.09551008)
        (68, 59.91027667)
        (69, 62.11107804)
        (70, 63.83170563)
        (71, 61.72796092)
        (72, 62.42255719)
        (73, 62.41608716)
        (74, 62.85008559)
        (75, 63.86275977)
        (76, 61.64633619)
        (77, 61.86763431)
        (78, 63.300419)
        (79, 70.61216299)
        (80, 71.30490621)
        (81, 69.17115969)
        (82, 71.73626531)
        (83, 73.22761354)
        (84, 67.19986543)
        (85, 73.53397362)
        (86, 76.75693789)
        (87, 75.24787807)
        (88, 73.65530039)
        (89, 77.50192905)
        (90, 75.94842153)
        (91, 77.12477113)
        (92, 77.12126478)
        (93, 76.71422997)
        (94, 77.95088847)
        (95, 79.2228139)
        (96, 77.41345895)
        (97, 81.95770308)
        (98, 80.44184872)
        (99, 82.03430774)
        (100, 78.57669283)
        (101, 82.48412344)
        (102, 80.78814024)
        (103, 83.86361613)
        (104, 85.34170442)
        (105, 86.05367642)
        (106, 86.03765818)
        (107, 85.12883448)
        (108, 81.64434285)
        (109, 85.96417874)
        (110, 86.41459805)
        (111, 87.66408809)
        (112, 83.83735179)
      };
      \end{axis}
    \end{tikzpicture}
  }
  \caption{Speedup curves for my MPI implementation}\label{fig:scaling}
\end{figure}

In general, my implementation initially scaled well for the smallest three grid sizes, but the speedup acquired from each subsequent process declined (i.e. a sublinear plateau), which occurred due to several reasons.
Perfect linear scaling was theoretically impossible because the program contained serial sections.

A different pattern emerged in the largest grid size.

Notably, the amount of speedup provided by each next core was approximately inversely proportional to the test case size.
In other words, larger grid sizes benefitted more from a multiprocess implementation than smaller grid sizes.
Firstly, this was because the larger grids benefitted more from being split sufficiently small to fit into the faster cache levels.
Secondly, the larger grid sizes were more evenly divided by the number of processes.

\section{Comparison to Serial}

\section{GPU Programming}

GPUs typically have 3--5x the memory bandwidth, and 5--10x the peak FLOP/s that CPUs have.
This is true for BC4, in which the Nvidia Pascal P100 has 4.8x the memory bandwidth and 9.8x the performance that the Intel E5-2680 v4 has.
OpenCL is a framework for heterogeneous computing that can be used for GPU programming.

I sought to identify whether I could produce an implementation of LBM in OpenCL to run on a single GPU in BCP4 that would be faster than my MPI implementation on one node.

\subsection{Original Code}

\begin{table}[htbp]
  \begin{center}
  \caption{Execution times with the OpenCL implementation and speedup over the serial implementation}\label{tab:OpenCL_1}
  \begin{tabular}[t]{l | l l} 
      \hline\hline
      Grid Size&Time (s)&Speedup\\
      \hline
      $128 \times 128$&\texttt{}&\texttt{}\\
      $128 \times 256$&\texttt{}&\texttt{}\\
      $256 \times 256$&\texttt{}&\texttt{}\\
      $1024 \times 1024$&\texttt{}&\texttt{}\\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{Optimisations}

\begin{table}[htbp]
  \begin{center}
  \caption{Execution times with the OpenCL implementation and speedup over the prior implementation}\label{tab:OpenCL_2}
  \begin{tabular}[t]{l | l l} 
      \hline\hline
      Grid Size&Time (s)&Speedup\\
      \hline
      $128 \times 128$&\texttt{}&\texttt{}\\
      $128 \times 256$&\texttt{}&\texttt{}\\
      $256 \times 256$&\texttt{}&\texttt{}\\
      $1024 \times 1024$&\texttt{}&\texttt{}\\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\section{Conclusion}

\printbibliography

\end{document}